{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bd63621",
   "metadata": {},
   "source": [
    "# 語意\n",
    "語意代表句子的意思。  \n",
    "在NLP內處理語意的手法如下:  \n",
    "1. Word Segmentation: 將詞語分割成正確的單詞。  \n",
    "2. Stemming: 將衍生字還原成原始型態。  \n",
    "3. Lemmatization: 將變形字轉成原始型態。  \n",
    "4. Part-of-Speech Tagging: 根據語言學辨識每一個字的詞性。\n",
    "5. Parsing: 根據語法樹辨識每一個字句的文法，並正確判斷句子的意思。  \n",
    "6. Sentence Breaking: 將文章內的句子完整拆分出來，又稱為sentence tokenize。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c4feee",
   "metadata": {},
   "source": [
    "### 1. Word Segmentation\n",
    "分詞常用套件: jieba(中文) , NLTK(英文)  \n",
    "\n",
    "> jieba  \n",
    "- cut函數可以用來做中文詞語分割  \n",
    "- add_word函數可以用來自訂詞語  \n",
    "- load_userdict函數可以載入自定義詞庫  \n",
    "- lcut函數可以將切完的結果以上list型別回傳\n",
    "\n",
    "> Ckip  \n",
    "- 台灣中研院開發的中文斷詞套件  \n",
    "- 支援多種NLP功能(POS,NER,...etc)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c337933c",
   "metadata": {},
   "source": [
    "##### jieba\n",
    "jieba的斷詞技術base on字典內的詞產生Trie Tree，並且根據Trie Tree建立DAG。  \n",
    "如果出現不在字典內的詞，則使用HMM方法來進行斷詞辨識。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e444cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1039b47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "輸入： 足球運動需要大家一起來推廣，歡迎加入我們的行列！\n"
     ]
    }
   ],
   "source": [
    "sentence = \"足球運動需要大家一起來推廣，歡迎加入我們的行列！\"\n",
    "print(\"輸入： {}\".format(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e2d2e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "words1 = jieba.cut(sentence, cut_all=False) # 速度快\n",
    "words2 = jieba.cut(sentence, cut_all=True) # 精確模式\n",
    "words3 = jieba.cut_for_search(sentence) # 搜尋引擎模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a2b149c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "精確模式：\n",
      "足球/運動/需要/大家/一起/來/推廣/，/歡迎/加入/我們/的/行列/！/\n",
      "全模式：\n",
      "足球/運/動/需要/大家/一起/來/推/廣/，/歡/迎/加入/我/們/的/行列/！/\n",
      "搜索引擎模式：\n",
      "足球/運動/需要/大家/一起/來/推廣/，/歡迎/加入/我們/的/行列/！/"
     ]
    }
   ],
   "source": [
    "print(\"精確模式：\")\n",
    "for word in words1:\n",
    "    print(word+'/', end='')\n",
    "\n",
    "print(\"\\n全模式：\")\n",
    "for word in words2:\n",
    "    print(word+'/', end='')\n",
    "\n",
    "print(\"\\n搜索引擎模式：\")\n",
    "for word in words3:\n",
    "    print(word+'/', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2fc27cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "考試/即將/結束/"
     ]
    }
   ],
   "source": [
    "text = '考試即將結束'\n",
    "words4 = jieba.cut(text, cut_all=False)\n",
    "for word in words4:\n",
    "    print(word+'/', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a5340a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "考試/即將結束/"
     ]
    }
   ],
   "source": [
    "# 若要將\"即將\"與\"結束\"當作一個斷詞，可使用load_userdict函數搭配自訂dict.txt 或者 add_word自行加入斷詞\n",
    "jieba.add_word('即將結束', freq=None, tag=None)\n",
    "\n",
    "# 測試斷詞\n",
    "text = '考試即將結束'\n",
    "words4 = jieba.cut(text, cut_all=False)\n",
    "for word in words4:\n",
    "    print(word+'/', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a6c31ae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('足球', 0, 2),\n",
       " ('運動', 2, 4),\n",
       " ('需要', 4, 6),\n",
       " ('大家', 6, 8),\n",
       " ('一起', 8, 10),\n",
       " ('來', 10, 11),\n",
       " ('推廣', 11, 13),\n",
       " ('，', 13, 14),\n",
       " ('歡迎', 14, 16),\n",
       " ('加入', 16, 18),\n",
       " ('我們', 18, 20),\n",
       " ('的', 20, 21),\n",
       " ('行列', 21, 23),\n",
       " ('！', 23, 24)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# jieba 的 tokenize 可以將每一個斷詞的起始與結束點記錄下來\n",
    "import jieba\n",
    "sentence = \"足球運動需要大家一起來推廣，歡迎加入我們的行列！\"\n",
    "words = jieba.tokenize(sentence)\n",
    "list(words)\n",
    "# for tk in words:\n",
    "#     print(\"word {}\\t\\t start: {} \\t\\t end:{}\".format(tk[0],tk[1],tk[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c10405",
   "metadata": {},
   "source": [
    "##### Ckip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e3f6105",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ckiptagger import data_utils , construct_dictionary , WS , POS , NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f610bf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 從中研院遠端server download資料\n",
    "data_utils.download_data_url(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b46d99b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutText():\n",
    "    ws = WS(\"./data\")\n",
    "    pos = POS(\"./data\")\n",
    "    ner = NER(\"./data\")\n",
    "    word_to_weight = {\n",
    "        \n",
    "    }\n",
    "    dictionary = construct_dictionary(word_to_weight)\n",
    "    print(dictionary)\n",
    "    \n",
    "    sentence_list = [\"指揮中心表示，今日新增之26943例本土病例，為12577例男性、14350例女性、16例調查中\" , \n",
    "                     \"被軍事迷戲稱「妖怪」的中共山東號航空母艦，昨天被記錄到現身台海中線以西，沿中國沿海南下通過金門外海，隨後跟著一艘疑似補給艦，也疑似被在西南空域盤旋的美國空軍RC-135鎖定目標，畫面也被記錄下來。\" , \n",
    "                     \"前鋒格林（JaMychal Green）在選秀夜當天遭金塊交易至雷霆，7月20日則傳出他和雷霆完成買斷，並且將加盟勇士，為衛冕軍補充禁區戰力。\"]\n",
    "    \n",
    "    word_sentence_list = ws(sentence_list)\n",
    "    pos_sentence_list = pos(word_sentence_list)\n",
    "    entity_sentence_list = ner(word_sentence_list , pos_sentence_list)\n",
    "    \n",
    "    del ws\n",
    "    del pos\n",
    "    del ner\n",
    "    \n",
    "    def print_word_pos_sentence(word_sentence , pos_sentence):\n",
    "        for word , pos in zip(word_sentence , pos_sentence):\n",
    "            print(f\"{word}({pos})\" , end=\"\\u3000\")\n",
    "            print(\"\\n\")\n",
    "            \n",
    "    for i , sentence in enumerate(sentence_list):\n",
    "        print(\"\\n\")\n",
    "        print(f\"'{sentence}'\")\n",
    "        print_word_pos_sentence(word_sentence_list[i] , pos_sentence_list[i])\n",
    "        for entity in sorted(entity_sentence_list):\n",
    "            print(entity)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba89edba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "\n",
      "\n",
      "'指揮中心表示，今日新增之26943例本土病例，為12577例男性、14350例女性、16例調查中'\n",
      "指揮(Na)　\n",
      "\n",
      "中心(Nc)　\n",
      "\n",
      "表示(VE)　\n",
      "\n",
      "，(COMMACATEGORY)　\n",
      "\n",
      "今日(Nd)　\n",
      "\n",
      "新增(VJ)　\n",
      "\n",
      "之(DE)　\n",
      "\n",
      "26943(Neu)　\n",
      "\n",
      "例(Na)　\n",
      "\n",
      "本土(Nc)　\n",
      "\n",
      "病例(Na)　\n",
      "\n",
      "，(COMMACATEGORY)　\n",
      "\n",
      "為(VG)　\n",
      "\n",
      "12577(Neu)　\n",
      "\n",
      "例(Na)　\n",
      "\n",
      "男性(Na)　\n",
      "\n",
      "、(PAUSECATEGORY)　\n",
      "\n",
      "14350(Neu)　\n",
      "\n",
      "例(Na)　\n",
      "\n",
      "女性(Na)　\n",
      "\n",
      "、(PAUSECATEGORY)　\n",
      "\n",
      "16(Neu)　\n",
      "\n",
      "例(Na)　\n",
      "\n",
      "調查(VE)　\n",
      "\n",
      "中(Ng)　\n",
      "\n",
      "{(42, 44, 'CARDINAL', '16'), (33, 38, 'CARDINAL', '14350'), (12, 17, 'CARDINAL', '26943'), (24, 29, 'CARDINAL', '12577')}\n",
      "{(45, 47, 'GPE', '金門'), (74, 76, 'GPE', '美國'), (78, 84, 'PRODUCT', 'RC-135'), (29, 31, 'LOC', '台海'), (11, 13, 'ORG', '中共'), (21, 23, 'DATE', '昨天'), (37, 39, 'GPE', '中國'), (54, 55, 'CARDINAL', '一')}\n",
      "{(35, 40, 'DATE', '7月20日'), (21, 24, 'PERSON', '選秀夜'), (32, 34, 'PERSON', '雷霆'), (5, 19, 'PERSON', 'JaMychal Green'), (2, 4, 'PERSON', '格林'), (24, 26, 'DATE', '當天')}\n",
      "\n",
      "\n",
      "'被軍事迷戲稱「妖怪」的中共山東號航空母艦，昨天被記錄到現身台海中線以西，沿中國沿海南下通過金門外海，隨後跟著一艘疑似補給艦，也疑似被在西南空域盤旋的美國空軍RC-135鎖定目標，畫面也被記錄下來。'\n",
      "被(P)　\n",
      "\n",
      "軍事迷(Na)　\n",
      "\n",
      "戲稱(VE)　\n",
      "\n",
      "「(PARENTHESISCATEGORY)　\n",
      "\n",
      "妖怪(Na)　\n",
      "\n",
      "」(PARENTHESISCATEGORY)　\n",
      "\n",
      "的(DE)　\n",
      "\n",
      "中共(Nb)　\n",
      "\n",
      "山東號(Nb)　\n",
      "\n",
      "航空母艦(Na)　\n",
      "\n",
      "，(COMMACATEGORY)　\n",
      "\n",
      "昨天(Nd)　\n",
      "\n",
      "被(P)　\n",
      "\n",
      "記錄到(VE)　\n",
      "\n",
      "現身(VA)　\n",
      "\n",
      "台海(Nc)　\n",
      "\n",
      "中線(Na)　\n",
      "\n",
      "以西(Ncd)　\n",
      "\n",
      "，(COMMACATEGORY)　\n",
      "\n",
      "沿(P)　\n",
      "\n",
      "中國(Nc)　\n",
      "\n",
      "沿海(Nc)　\n",
      "\n",
      "南下(VCL)　\n",
      "\n",
      "通過(VCL)　\n",
      "\n",
      "金門(Nc)　\n",
      "\n",
      "外海(Nc)　\n",
      "\n",
      "，(COMMACATEGORY)　\n",
      "\n",
      "隨後(Nd)　\n",
      "\n",
      "跟(VC)　\n",
      "\n",
      "著(Di)　\n",
      "\n",
      "一(Neu)　\n",
      "\n",
      "艘(Nf)　\n",
      "\n",
      "疑似(VG)　\n",
      "\n",
      "補給艦(Na)　\n",
      "\n",
      "，(COMMACATEGORY)　\n",
      "\n",
      "也(D)　\n",
      "\n",
      "疑似(VG)　\n",
      "\n",
      "被(P)　\n",
      "\n",
      "在(P)　\n",
      "\n",
      "西南(Ncd)　\n",
      "\n",
      "空域(Na)　\n",
      "\n",
      "盤旋(VA)　\n",
      "\n",
      "的(DE)　\n",
      "\n",
      "美國(Nc)　\n",
      "\n",
      "空軍(Nc)　\n",
      "\n",
      "RC-135(FW)　\n",
      "\n",
      "鎖定(VC)　\n",
      "\n",
      "目標(Na)　\n",
      "\n",
      "，(COMMACATEGORY)　\n",
      "\n",
      "畫面(Na)　\n",
      "\n",
      "也(D)　\n",
      "\n",
      "被(P)　\n",
      "\n",
      "記錄下來(VB)　\n",
      "\n",
      "。(PERIODCATEGORY)　\n",
      "\n",
      "{(42, 44, 'CARDINAL', '16'), (33, 38, 'CARDINAL', '14350'), (12, 17, 'CARDINAL', '26943'), (24, 29, 'CARDINAL', '12577')}\n",
      "{(45, 47, 'GPE', '金門'), (74, 76, 'GPE', '美國'), (78, 84, 'PRODUCT', 'RC-135'), (29, 31, 'LOC', '台海'), (11, 13, 'ORG', '中共'), (21, 23, 'DATE', '昨天'), (37, 39, 'GPE', '中國'), (54, 55, 'CARDINAL', '一')}\n",
      "{(35, 40, 'DATE', '7月20日'), (21, 24, 'PERSON', '選秀夜'), (32, 34, 'PERSON', '雷霆'), (5, 19, 'PERSON', 'JaMychal Green'), (2, 4, 'PERSON', '格林'), (24, 26, 'DATE', '當天')}\n",
      "\n",
      "\n",
      "'前鋒格林（JaMychal Green）在選秀夜當天遭金塊交易至雷霆，7月20日則傳出他和雷霆完成買斷，並且將加盟勇士，為衛冕軍補充禁區戰力。'\n",
      "前鋒(Na)　\n",
      "\n",
      "格林(Nb)　\n",
      "\n",
      "（(PARENTHESISCATEGORY)　\n",
      "\n",
      "Ja(FW)　\n",
      "\n",
      "Mychal (FW)　\n",
      "\n",
      "Green(FW)　\n",
      "\n",
      "）(PARENTHESISCATEGORY)　\n",
      "\n",
      "在(P)　\n",
      "\n",
      "選秀夜(Nd)　\n",
      "\n",
      "當天(Nd)　\n",
      "\n",
      "遭(P)　\n",
      "\n",
      "金塊(Na)　\n",
      "\n",
      "交易(Na)　\n",
      "\n",
      "至(P)　\n",
      "\n",
      "雷霆，(Nb)　\n",
      "\n",
      "7月(Nd)　\n",
      "\n",
      "20日(Nd)　\n",
      "\n",
      "則(D)　\n",
      "\n",
      "傳出(VC)　\n",
      "\n",
      "他(Nh)　\n",
      "\n",
      "和(P)　\n",
      "\n",
      "雷霆(Na)　\n",
      "\n",
      "完成(VC)　\n",
      "\n",
      "買斷(VC)　\n",
      "\n",
      "，(COMMACATEGORY)　\n",
      "\n",
      "並且(Cbb)　\n",
      "\n",
      "將(D)　\n",
      "\n",
      "加盟(VC)　\n",
      "\n",
      "勇士(Na)　\n",
      "\n",
      "，(COMMACATEGORY)　\n",
      "\n",
      "為(P)　\n",
      "\n",
      "衛冕軍(Na)　\n",
      "\n",
      "補充(VC)　\n",
      "\n",
      "禁區(Nc)　\n",
      "\n",
      "戰力(Na)　\n",
      "\n",
      "。(PERIODCATEGORY)　\n",
      "\n",
      "{(42, 44, 'CARDINAL', '16'), (33, 38, 'CARDINAL', '14350'), (12, 17, 'CARDINAL', '26943'), (24, 29, 'CARDINAL', '12577')}\n",
      "{(45, 47, 'GPE', '金門'), (74, 76, 'GPE', '美國'), (78, 84, 'PRODUCT', 'RC-135'), (29, 31, 'LOC', '台海'), (11, 13, 'ORG', '中共'), (21, 23, 'DATE', '昨天'), (37, 39, 'GPE', '中國'), (54, 55, 'CARDINAL', '一')}\n",
      "{(35, 40, 'DATE', '7月20日'), (21, 24, 'PERSON', '選秀夜'), (32, 34, 'PERSON', '雷霆'), (5, 19, 'PERSON', 'JaMychal Green'), (2, 4, 'PERSON', '格林'), (24, 26, 'DATE', '當天')}\n"
     ]
    }
   ],
   "source": [
    "cutText()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3732b11",
   "metadata": {},
   "source": [
    "### 2. Stemming\n",
    "常用套件: Porter、Snowball、Lancaster(英文)  \n",
    "\n",
    "將衍生字轉成原始型態，例如:  \n",
    "plays/playing/played => play  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05151c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eat\n",
      "pass\n",
      "put\n",
      "=================\n",
      "eat\n",
      "pass\n",
      "put\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "#nltk.download('all')\n",
    "pst = PorterStemmer()\n",
    "snowball = SnowballStemmer('english')\n",
    "\n",
    "print(pst.stem('eating'))\n",
    "print(pst.stem('passed'))\n",
    "print(pst.stem('puts'))\n",
    "\n",
    "print(\"=================\")\n",
    "\n",
    "print(snowball.stem('eating'))\n",
    "print(snowball.stem('passed'))\n",
    "print(snowball.stem('puts'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b86f7e7",
   "metadata": {},
   "source": [
    "### 3. Lemmatization\n",
    "常用套件: WordNet(英文)  \n",
    "\n",
    "將變形字轉成原始型態，例如:  \n",
    "is/are/been => be  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4d6c530",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Student\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00165f6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Student\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index\n",
      "struggle\n",
      "sad\n"
     ]
    }
   ],
   "source": [
    "# 後面要指定詞性\n",
    "nltk.download('omw-1.4')\n",
    "print(wnl.lemmatize('indexes','n'))  # noun名詞\n",
    "print(wnl.lemmatize('struggling','v')) # verb動詞\n",
    "print(wnl.lemmatize('saddest', 'a')) # adj形容詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24442010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('car.n.01')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# 找出詞語屬於 wordnet 裡面哪一種 syn 分類\n",
    "wn.synsets('motorcar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3df67666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('trunk.n.01'),\n",
       " Synset('trunk.n.02'),\n",
       " Synset('torso.n.01'),\n",
       " Synset('luggage_compartment.n.01'),\n",
       " Synset('proboscis.n.02')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('trunk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "675e3de8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['car', 'auto', 'automobile', 'machine', 'motorcar']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show出每個car.n.01這個 wordnet syn 類別類包含哪些同意單詞\n",
    "wn.synset('car.n.01').lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "148fdaa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['trunk', 'tree_trunk', 'bole']\n",
      "['trunk']\n",
      "['torso', 'trunk', 'body']\n",
      "['luggage_compartment', 'automobile_trunk', 'trunk']\n",
      "['proboscis', 'trunk']\n"
     ]
    }
   ],
   "source": [
    "# 單字 trunk 有很多意思，我們把不同意義的trunk，其同意詞show出來\n",
    "for synset in wn.synsets('trunk'):\n",
    "    print(synset.lemma_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7cf47cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a motor vehicle with four wheels; usually propelled by an internal combustion engine'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查詢 car.n.01 這個分類在WordNet裡面的定義\n",
    "wn.synset('car.n.01').definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ab1002a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the main stem of a tree; usually covered with bark; the bole is usually the part that is commercially useful for lumber'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查詢 trunk.n.01 這個分類在WordNet裡面的定義\n",
    "wn.synset('trunk.n.01').definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41746b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('motor_vehicle.n.01')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 找出 car.n.01 分類的上位分類 hyper = 上\n",
    "motorcar = wn.synset('car.n.01')\n",
    "types_of_motorcar = motorcar.hypernyms()\n",
    "types_of_motorcar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72fb7266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('ambulance.n.01'),\n",
       " Synset('beach_wagon.n.01'),\n",
       " Synset('bus.n.04'),\n",
       " Synset('cab.n.03'),\n",
       " Synset('compact.n.03'),\n",
       " Synset('convertible.n.01'),\n",
       " Synset('coupe.n.01'),\n",
       " Synset('cruiser.n.01'),\n",
       " Synset('electric.n.01'),\n",
       " Synset('gas_guzzler.n.01'),\n",
       " Synset('hardtop.n.01'),\n",
       " Synset('hatchback.n.01'),\n",
       " Synset('horseless_carriage.n.01'),\n",
       " Synset('hot_rod.n.01'),\n",
       " Synset('jeep.n.01'),\n",
       " Synset('limousine.n.01'),\n",
       " Synset('loaner.n.02'),\n",
       " Synset('minicar.n.01'),\n",
       " Synset('minivan.n.01'),\n",
       " Synset('model_t.n.01'),\n",
       " Synset('pace_car.n.01'),\n",
       " Synset('racer.n.02'),\n",
       " Synset('roadster.n.01'),\n",
       " Synset('sedan.n.01'),\n",
       " Synset('sport_utility.n.01'),\n",
       " Synset('sports_car.n.01'),\n",
       " Synset('stanley_steamer.n.01'),\n",
       " Synset('stock_car.n.01'),\n",
       " Synset('subcompact.n.01'),\n",
       " Synset('touring_car.n.01'),\n",
       " Synset('used-car.n.01')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 找出 car.n.01 分類的下位分類\n",
    "motorcar = wn.synset('car.n.01')\n",
    "types_of_motorcar = motorcar.hyponyms()\n",
    "types_of_motorcar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "418df245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Model_T',\n",
       " 'S.U.V.',\n",
       " 'SUV',\n",
       " 'Stanley_Steamer',\n",
       " 'ambulance',\n",
       " 'beach_waggon',\n",
       " 'beach_wagon',\n",
       " 'bus',\n",
       " 'cab',\n",
       " 'compact',\n",
       " 'compact_car',\n",
       " 'convertible',\n",
       " 'coupe',\n",
       " 'cruiser',\n",
       " 'electric',\n",
       " 'electric_automobile',\n",
       " 'electric_car',\n",
       " 'estate_car',\n",
       " 'gas_guzzler',\n",
       " 'hack',\n",
       " 'hardtop',\n",
       " 'hatchback',\n",
       " 'heap',\n",
       " 'horseless_carriage',\n",
       " 'hot-rod',\n",
       " 'hot_rod',\n",
       " 'jalopy',\n",
       " 'jeep',\n",
       " 'landrover',\n",
       " 'limo',\n",
       " 'limousine',\n",
       " 'loaner',\n",
       " 'minicar',\n",
       " 'minivan',\n",
       " 'pace_car',\n",
       " 'patrol_car',\n",
       " 'phaeton',\n",
       " 'police_car',\n",
       " 'police_cruiser',\n",
       " 'prowl_car',\n",
       " 'race_car',\n",
       " 'racer',\n",
       " 'racing_car',\n",
       " 'roadster',\n",
       " 'runabout',\n",
       " 'saloon',\n",
       " 'secondhand_car',\n",
       " 'sedan',\n",
       " 'sport_car',\n",
       " 'sport_utility',\n",
       " 'sport_utility_vehicle',\n",
       " 'sports_car',\n",
       " 'squad_car',\n",
       " 'station_waggon',\n",
       " 'station_wagon',\n",
       " 'stock_car',\n",
       " 'subcompact',\n",
       " 'subcompact_car',\n",
       " 'taxi',\n",
       " 'taxicab',\n",
       " 'tourer',\n",
       " 'touring_car',\n",
       " 'two-seater',\n",
       " 'used-car',\n",
       " 'waggon',\n",
       " 'wagon']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 找到下位詞組後後，再從 synset 找出單詞（以詞為中心）\n",
    "sorted( lemma.name() for synset in types_of_motorcar for lemma in synset.lemmas() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "abfb79da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Synset('entity.n.01'),\n",
       "  Synset('physical_entity.n.01'),\n",
       "  Synset('object.n.01'),\n",
       "  Synset('whole.n.02'),\n",
       "  Synset('artifact.n.01'),\n",
       "  Synset('instrumentality.n.03'),\n",
       "  Synset('container.n.01'),\n",
       "  Synset('wheeled_vehicle.n.01'),\n",
       "  Synset('self-propelled_vehicle.n.01'),\n",
       "  Synset('motor_vehicle.n.01'),\n",
       "  Synset('car.n.01')],\n",
       " [Synset('entity.n.01'),\n",
       "  Synset('physical_entity.n.01'),\n",
       "  Synset('object.n.01'),\n",
       "  Synset('whole.n.02'),\n",
       "  Synset('artifact.n.01'),\n",
       "  Synset('instrumentality.n.03'),\n",
       "  Synset('conveyance.n.03'),\n",
       "  Synset('vehicle.n.01'),\n",
       "  Synset('wheeled_vehicle.n.01'),\n",
       "  Synset('self-propelled_vehicle.n.01'),\n",
       "  Synset('motor_vehicle.n.01'),\n",
       "  Synset('car.n.01')]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 顯示完整路徑（上位詞組再往上走）\n",
    "# 可以看到最原始的詞組就是第一個陣列內的元素(例如: entity,object...etc)\n",
    "motorcar = wn.synset('car.n.01')\n",
    "motorcar.hypernym_paths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba9a873c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('entity.n.01')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 直指頂端上位詞組\n",
    "motorcar = wn.synset('car.n.01')\n",
    "motorcar.root_hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6fdea422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以鯨魚為例\n",
    "right = wn.synset(\"right_whale.n.01\")\n",
    "minke = wn.synset(\"minke_whale.n.01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1212e098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('baleen_whale.n.01')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 「露脊鯨」與「小鬚鯨」在上位詞組中最低位的詞組 (共同最靠近的詞組)\n",
    "right.lowest_common_hypernyms(minke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3cb83e4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('whale.n.02')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 露脊鯨 vs 虎鯨\n",
    "orca = wn.synset(\"orca.n.01\")\n",
    "right.lowest_common_hypernyms(orca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "deda9d99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('vertebrate.n.01')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 露脊鯨 vs 陸龜\n",
    "# 因為兩物種差比較遠，所以顯示結果就不是鯨類，而是更上層的分類: 脊椎動物(vertebrate)\n",
    "tortoise = wn.synset(\"tortoise.n.01\")\n",
    "right.lowest_common_hypernyms(tortoise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ba85f64f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('entity.n.01')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 露脊鯨 vs 小說\n",
    "novel = wn.synset(\"novel.n.01\")\n",
    "right.lowest_common_hypernyms(novel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7e1ef040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "13\n",
      "8\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# 計算由當前 synset 而上的階層數 (離 root 幾層)\n",
    "print(wn.synset('baleen_whale.n.01').min_depth())\n",
    "print(wn.synset('whale.n.02').min_depth())\n",
    "print(wn.synset('vertebrate.n.01').min_depth())\n",
    "print(wn.synset('entity.n.01').min_depth())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5c574749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.25\n",
      "0.16666666666666666\n",
      "0.07692307692307693\n",
      "0.043478260869565216\n"
     ]
    }
   ],
   "source": [
    "# 上下位詞組結構的相似程度 (數字接近1代表path越像)\n",
    "print(right.path_similarity(right))     #露脊鯨和自己本身\n",
    "print(right.path_similarity(minke))     #露脊鯨和小鬚鯨\n",
    "print(right.path_similarity(orca))      #露脊鯨和虎鯨\n",
    "print(right.path_similarity(tortoise))  #露脊鯨和陸龜\n",
    "print(right.path_similarity(novel))     #露脊鯨和小說"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7328a026",
   "metadata": {},
   "source": [
    "### 4. Part-of-Speech Tagging\n",
    "常用套件: jieba(中文)、Ckip(中文)、NLTK(英文)  \n",
    "\n",
    "根據語言學辨別每一個字的詞性，例如: 動詞/名詞/主詞... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "463de0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'The brown fox is quick and he is jumping over the lazy dog'\n",
    "tokens = nltk.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e36197cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Student\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('brown', 'JJ'), ('fox', 'NN'), ('is', 'VBZ'), ('quick', 'JJ'), ('and', 'CC'), ('he', 'PRP'), ('is', 'VBZ'), ('jumping', 'VBG'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "# 將句子分詞後判斷每一個字元的詞性\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "tagged_sent = nltk.pos_tag(tokens)\n",
    "print(tagged_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eaeb3cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mgutenberg\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('gutenberg')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/gutenberg\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Student/nltk_data'\n    - 'C:\\\\Users\\\\Student\\\\anaconda3\\\\envs\\\\NLP\\\\nltk_data'\n    - 'C:\\\\Users\\\\Student\\\\anaconda3\\\\envs\\\\NLP\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Student\\\\anaconda3\\\\envs\\\\NLP\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Student\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\NLP\\lib\\site-packages\\nltk\\corpus\\util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NLP\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mgutenberg\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('gutenberg')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/gutenberg.zip/gutenberg/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Student/nltk_data'\n    - 'C:\\\\Users\\\\Student\\\\anaconda3\\\\envs\\\\NLP\\\\nltk_data'\n    - 'C:\\\\Users\\\\Student\\\\anaconda3\\\\envs\\\\NLP\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Student\\\\anaconda3\\\\envs\\\\NLP\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Student\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 不分詞直接判斷，則使用pos_tag_sents()\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# (錯誤作法!)\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m  \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      5\u001b[0m tagged_sent \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mpos_tag_sents(sentence)  \u001b[38;5;66;03m# 必須是 list of string，不能只放單一string\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(tagged_sent)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NLP\\lib\\site-packages\\nltk\\book.py:27\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mType the name of the text or sentence to view it.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mType: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtexts()\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msents()\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to list the materials.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 27\u001b[0m text1 \u001b[38;5;241m=\u001b[39m Text(\u001b[43mgutenberg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwords\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmelville-moby_dick.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext1:\u001b[39m\u001b[38;5;124m\"\u001b[39m, text1\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m     30\u001b[0m text2 \u001b[38;5;241m=\u001b[39m Text(gutenberg\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mausten-sense.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NLP\\lib\\site-packages\\nltk\\corpus\\util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NLP\\lib\\site-packages\\nltk\\corpus\\util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NLP\\lib\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NLP\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mgutenberg\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('gutenberg')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/gutenberg\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Student/nltk_data'\n    - 'C:\\\\Users\\\\Student\\\\anaconda3\\\\envs\\\\NLP\\\\nltk_data'\n    - 'C:\\\\Users\\\\Student\\\\anaconda3\\\\envs\\\\NLP\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Student\\\\anaconda3\\\\envs\\\\NLP\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Student\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# 不分詞直接判斷，則使用pos_tag_sents()\n",
    "\n",
    "# (錯誤作法!)\n",
    "from nltk.book import  *\n",
    "tagged_sent = nltk.pos_tag_sents(sentence)  # 必須是 list of string，不能只放單一string\n",
    "print(tagged_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d10efdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Eligibility', 'NN'), ('to', 'TO'), ('receive', 'VB'), ('a', 'DT'), ('second', 'JJ'), ('COVID-19', 'JJ'), ('vaccine', 'NN'), ('booster', 'NN'), ('is', 'VBZ'), ('to', 'TO'), ('be', 'VB'), ('expanded', 'VBN'), ('to', 'TO'), ('include', 'VB'), ('adults', 'NNS'), ('aged', 'VBN'), ('50', 'CD'), ('or', 'CC'), ('older', 'JJR'), ('from', 'IN'), ('tomorrow', 'NN'), (',', ','), ('the', 'DT'), ('Central', 'NNP'), ('Epidemic', 'NNP'), ('Command', 'NNP'), ('Center', 'NNP'), ('(', '('), ('CECC', 'NNP'), (')', ')'), ('said', 'VBD'), ('yesterday', 'NN'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "# (正確做法!)\n",
    "text1 = 'European Parliament Vice President Nicola Beer in Taipei yesterday urged China to refrain from “threatening gestures” that could alter the “status quo” in the Taiwan Strait.'\n",
    "text2 = 'Eligibility to receive a second COVID-19 vaccine booster is to be expanded to include adults aged 50 or older from tomorrow, the Central Epidemic Command Center (CECC) said yesterday.'\n",
    "tagged_sent = nltk.pos_tag_sents(nltk.word_tokenize(sent) for sent in nltk.sent_tokenize(text2))\n",
    "print(tagged_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a6891846",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Student\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.459 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "指揮/v\n",
      "中心/n\n",
      "表示/v\n",
      "，/x\n",
      "今日/t\n",
      "新增/v\n",
      "之/f\n",
      "26/m\n",
      ",/x\n",
      "943/m\n",
      "例/v\n",
      "本土/n\n",
      "病例/n\n",
      "，/x\n",
      "為/p\n",
      "12/m\n",
      ",/x\n",
      "577/m\n",
      "例/v\n",
      "男性/n\n",
      "、/x\n",
      "14/m\n",
      ",/x\n",
      "350/m\n",
      "例/v\n",
      "女性/n\n",
      "、/x\n",
      "16/m\n",
      "例/n\n",
      "調查/vn\n",
      "中/f\n"
     ]
    }
   ],
   "source": [
    "import jieba.posseg\n",
    "result = jieba.posseg.cut(\"指揮中心表示，今日新增之26,943例本土病例，為12,577例男性、14,350例女性、16例調查中\")\n",
    "for x in result:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a149d0a",
   "metadata": {},
   "source": [
    "### 5. Parsing\n",
    "常用套件: gensim\n",
    "\n",
    "根據語法樹辨別一個字句的文法，並做出正確判斷句子是哪種意思。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed57ad61",
   "metadata": {},
   "source": [
    "<img src=\"./NLP_parsing.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749bc5ea",
   "metadata": {},
   "source": [
    "### 6. Sentence Breaking\n",
    "常用套件: spacy(英文)、NLTK(英文)\n",
    "\n",
    "將斷落或文章內的句子拆分出來。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06933388",
   "metadata": {},
   "source": [
    "##### NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2b94ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backgammon is one of the oldest known board games.\n",
      "\n",
      "Its history can be traced back nearly 5,000 years to archeological discoveries in the Middle East.\n",
      "\n",
      "It is a two player game where each player has fifteen checkers which move between twenty-four points according to the roll of two dice.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "text = 'Backgammon is one of the oldest known board games. Its history can be traced back nearly 5,000 years to archeological discoveries in the Middle East. It is a two player game where each player has fifteen checkers which move between twenty-four points according to the roll of two dice.'\n",
    "sentences = sent_tokenize(text)\n",
    "for sentence in sentences:\n",
    "    print(sentence+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "47ef13a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 整合範例 : word_tokenize + pos_tag + lemmatize\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "sentence = 'football is a family of team sports that involve, to varying degrees, kicking a ball to score a goal.'\n",
    "tokens = word_tokenize(sentence)\n",
    "tagged_sent = pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7382ac72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['football', 'is', 'a', 'family', 'of', 'team', 'sports', 'that', 'involve', ',', 'to', 'varying', 'degrees', ',', 'kicking', 'a', 'ball', 'to', 'score', 'a', 'goal', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b723b58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('football', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('family', 'NN'), ('of', 'IN'), ('team', 'NN'), ('sports', 'NNS'), ('that', 'WDT'), ('involve', 'VBP'), (',', ','), ('to', 'TO'), ('varying', 'VBG'), ('degrees', 'NNS'), (',', ','), ('kicking', 'VBG'), ('a', 'DT'), ('ball', 'NN'), ('to', 'TO'), ('score', 'VB'), ('a', 'DT'), ('goal', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(tagged_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "08e0e34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['football', 'be', 'a', 'family', 'of', 'team', 'sport', 'that', 'involve', ',', 'to', 'vary', 'degree', ',', 'kick', 'a', 'ball', 'to', 'score', 'a', 'goal', '.']\n"
     ]
    }
   ],
   "source": [
    "# 可以看到 sports 被還原成 sport ； is 被還原成 be\n",
    "wnl = WordNetLemmatizer()\n",
    "lemmas_sent = []\n",
    "for tag in tagged_sent:\n",
    "    wordnet_pos = get_wordnet_pos(tag[1]) or wordnet.NOUN # 這邊要先取出詞性的原因是後面要還原的時候需要詞性做第2輸入\n",
    "    lemmas_sent.append(wnl.lemmatize(tag[0], pos=wordnet_pos)) # pos參數用到上一列的 wordnet_pos 參數\n",
    "print(lemmas_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615ed8df",
   "metadata": {},
   "source": [
    "##### spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4655bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All human beings are born free and equal in dignity and rights.\n",
      "They are endowed with reason and conscience and should act towards one another in a spirit of brotherhood.\n",
      "Everyone has the right to life, liberty and security of person.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "# 如果下面這句不能執行，記得在terminal執行 : python -m spacy download en\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(\"All human beings are born free and equal in dignity and rights. They are endowed with reason and conscience and should act towards one another in a spirit of brotherhood. Everyone has the right to life, liberty and security of person.\")\n",
    "\n",
    "# 分出句子\n",
    "for item in doc.sents:\n",
    "    print(item.text)\n",
    "    \n",
    "# 把句子存成list物件\n",
    "sentences_as_list = list(doc.sents)\n",
    "len(sentences_as_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b5de226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All all\n",
      "human human\n",
      "beings being\n",
      "are be\n",
      "born bear\n",
      "free free\n",
      "and and\n",
      "equal equal\n",
      "in in\n",
      "dignity dignity\n",
      "and and\n",
      "rights right\n",
      ". .\n",
      "They they\n",
      "are be\n",
      "endowed endow\n",
      "with with\n",
      "reason reason\n",
      "and and\n",
      "conscience conscience\n",
      "and and\n",
      "should should\n",
      "act act\n",
      "towards towards\n",
      "one one\n",
      "another another\n",
      "in in\n",
      "a a\n",
      "spirit spirit\n",
      "of of\n",
      "brotherhood brotherhood\n",
      ". .\n",
      "Everyone everyone\n",
      "has have\n",
      "the the\n",
      "right right\n",
      "to to\n",
      "life life\n",
      ", ,\n",
      "liberty liberty\n",
      "and and\n",
      "security security\n",
      "of of\n",
      "person person\n",
      ". .\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.choice(sentences_as_list)\n",
    "\n",
    "# 還原每一個詞語的同型態\n",
    "for word in doc:\n",
    "    print(word.text, word.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c64dd379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They\n",
      "are\n",
      "endowed\n",
      "with\n",
      "reason\n",
      "and\n",
      "conscience\n",
      "and\n",
      "should\n",
      "act\n",
      "towards\n",
      "one\n",
      "another\n",
      "in\n",
      "a\n",
      "spirit\n",
      "of\n",
      "brotherhood\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "sentence = list(doc.sents)[1]\n",
    "# show出句子內每一個單詞\n",
    "for word in sentence:\n",
    "    print(word.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97041a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All DET DT\n",
      "human ADJ JJ\n",
      "beings NOUN NNS\n",
      "are AUX VBP\n",
      "born VERB VBN\n",
      "free ADJ JJ\n",
      "and CCONJ CC\n",
      "equal ADJ JJ\n",
      "in ADP IN\n",
      "dignity NOUN NN\n",
      "and CCONJ CC\n",
      "rights NOUN NNS\n",
      ". PUNCT .\n",
      "They PRON PRP\n",
      "are AUX VBP\n",
      "endowed VERB VBN\n",
      "with ADP IN\n",
      "reason NOUN NN\n",
      "and CCONJ CC\n",
      "conscience NOUN NN\n",
      "and CCONJ CC\n",
      "should AUX MD\n",
      "act VERB VB\n",
      "towards ADP IN\n",
      "one NUM CD\n",
      "another DET DT\n",
      "in ADP IN\n",
      "a DET DT\n",
      "spirit NOUN NN\n",
      "of ADP IN\n",
      "brotherhood NOUN NN\n",
      ". PUNCT .\n",
      "Everyone PRON NN\n",
      "has VERB VBZ\n",
      "the DET DT\n",
      "right NOUN NN\n",
      "to ADP IN\n",
      "life NOUN NN\n",
      ", PUNCT ,\n",
      "liberty NOUN NN\n",
      "and CCONJ CC\n",
      "security NOUN NN\n",
      "of ADP IN\n",
      "person NOUN NN\n",
      ". PUNCT .\n"
     ]
    }
   ],
   "source": [
    "# show出每一個單詞的性質(詞語/詞性/tag)\n",
    "for item in doc:\n",
    "    print(item.text, item.pos_, item.tag_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "baa1e3ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['born', 'endowed', 'act', 'has']\n"
     ]
    }
   ],
   "source": [
    "# 篩選動詞的詞語\n",
    "verbs = []\n",
    "for item in doc:\n",
    "    if item.pos_ == 'VERB':\n",
    "        verbs.append(item.text)\n",
    "print(verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "656584b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John McCain\n",
      "the Apple Store\n",
      "Manhattan\n"
     ]
    }
   ],
   "source": [
    "doc2 = nlp(\"John McCain and I visited the Apple Store in Manhattan.\")\n",
    "# 辨識實體(entity)\n",
    "for item in doc2.ents:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77fff1b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John McCain PERSON\n",
      "the Apple Store ORG\n",
      "Manhattan GPE\n"
     ]
    }
   ],
   "source": [
    "# 查看辨識的entity為哪一種實體命名\n",
    "for item in doc2.ents:\n",
    "    print(item.text, item.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a40770",
   "metadata": {},
   "source": [
    "# 語法\n",
    "語法代表句子內的結構與詞語之間的關係。  \n",
    "在NLP內處理語法的手法如下:  \n",
    "1. Named Entity Recognition(NER): 標註文章內的字詞屬性(e.g. 時間、地點、組織...etc)  \n",
    "2. Word Sentence Disambiguation: 讀取上下文，給予某字詞合適的意思。  \n",
    "3. Natural Language Genreation(NLG): 根據歷史資料來生成文章。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e913c32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
